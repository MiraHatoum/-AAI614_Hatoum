{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiraHatoum/-AAI614_Hatoum/blob/main/notebook7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGKpI0ZNgoPn"
      },
      "source": [
        "# AAI614: Data Science & its Applications\n",
        "\n",
        "*Notebook 7.1: Introducing Dask*\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/harmanani/AAI614/blob/main/Week%207/Notebook7.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Source: NVIDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rGbZuhEgoP3"
      },
      "source": [
        "# Dask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzV2gq1CgoP7"
      },
      "source": [
        "Dask is not faster than pandas for a single file or for small size data.  It excels for multiple data as it uses lazy computaion. In this lab, we will learn how to use Dask to speed up computation under the correct conditions.\n",
        "\n",
        "First, let's get these libraries loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QBmJ7D8goP9",
        "outputId": "870c8cb5-acd0-48b6-f919-9010f4a18e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "!pip install dask\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import glob\n",
        "import pandas as pd\n",
        "import time\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CYiyjOgoQB"
      },
      "source": [
        "## Using Dask versus Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gucVWtSmgoQC"
      },
      "source": [
        "Neither pandas or cuDF can read in multiple CSV files directly with [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). In order to read multiple files into a DataFrame, we would need to loop through each file and append them together.\n",
        "\n",
        "To see this, let's pull a couple more files from the [Water Level Website](https://tidesandcurrents.noaa.gov/stations.html?type=Water+Levels). This time, we will request a CSV and save it with the [urllib.request](https://docs.python.org/3/library/urllib.request.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnSl3dvgoQF"
      },
      "source": [
        "We should now have a few `.csv` files in the `data` folder. When referencing these files, we could type out the paths of each of these files individually, but instead, we will use the [glob](https://docs.python.org/3/library/glob.html) library to programmatically do this for us. We can use `*` as a wild card to filter files that match our pattern specified like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EadTsMC6goQI",
        "outputId": "cb7698c3-e218-4ecb-a0a7-11ad01366108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory exists.\n",
            "Files in the directory: []\n",
            "No matching .csv files found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Define the folder path\n",
        "data_folder = r\"C:\\Users\\MiraHatoum\\Downloads\\data\"\n",
        "\n",
        "# Step 1: Verify folder existence and list files\n",
        "if os.path.exists(data_folder):\n",
        "    print(\"Directory exists.\")\n",
        "    files = os.listdir(data_folder)\n",
        "    print(\"Files in the directory:\", files)\n",
        "else:\n",
        "    print(f\"Directory does not exist: {data_folder}\")\n",
        "\n",
        "# Step 2: Add missing .csv extensions\n",
        "for file in os.listdir(data_folder):\n",
        "    file_path = os.path.join(data_folder, file)\n",
        "    if not file.endswith(\".csv\"):\n",
        "        new_file_path = file_path + \".csv\"\n",
        "        os.rename(file_path, new_file_path)\n",
        "        print(f\"Renamed: {file_path} to {new_file_path}\")\n",
        "\n",
        "# Step 3: Match .csv files using glob\n",
        "file_pattern = os.path.join(data_folder, \"*.csv\")\n",
        "csv_files = glob.glob(file_pattern)\n",
        "\n",
        "if csv_files:\n",
        "    print(f\"Found {len(csv_files)} CSV files:\")\n",
        "    for file in csv_files:\n",
        "        print(file)\n",
        "\n",
        "    # Step 4: Combine all .csv files into a single DataFrame\n",
        "    combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
        "    print(\"\\nCombined DataFrame:\")\n",
        "    print(combined_df.head())\n",
        "else:\n",
        "    print(\"No matching .csv files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huu4OmWygoQK"
      },
      "source": [
        "Each path starts with `data`, ends with `.csv`, and the `*` indicates to pick up anything in between. Let's set up a for loop to see how long it takes to read all of these files. Run the block **twice** to see how much faster cuDF is after it has been initialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zP-nUQigoQM",
        "outputId": "47317c5d-2fdd-497a-ae38-15e129194ded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No matching .csv files found.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Define the folder path and file pattern\n",
        "data_folder = r\"C:\\Users\\MiraHatoum\\Downloads\\data\"\n",
        "file_pattern = data_folder + r\"\\*.csv\"\n",
        "\n",
        "# Use glob to find all .csv files\n",
        "file_paths = glob.glob(file_pattern)\n",
        "\n",
        "if not file_paths:\n",
        "    print(\"No matching .csv files found.\")\n",
        "else:\n",
        "    print(f\"Found {len(file_paths)} CSV files:\")\n",
        "    for file in file_paths:\n",
        "        print(file)\n",
        "\n",
        "    # Define the function to read and combine CSV files\n",
        "    def read_all(library, file_paths):\n",
        "        df_list = []\n",
        "        for file in file_paths:\n",
        "            try:\n",
        "                df = library.read_csv(\n",
        "                    file,\n",
        "                    index_col=None,\n",
        "                    header=None,  # Change to `0` if files have headers\n",
        "                    usecols=[0, 1, 2, 4, 5],  # Select specific columns\n",
        "                    skiprows=1  # Skip the first row if it's not data\n",
        "                )\n",
        "                df_list.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file}: {e}\")\n",
        "        return library.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "    # Use pandas to read files\n",
        "    df_cpu = read_all(pd, file_paths)\n",
        "\n",
        "    # Display the combined DataFrame\n",
        "    print(\"\\nCombined DataFrame:\")\n",
        "    print(df_cpu.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QR7MhongoQN",
        "outputId": "6eb4622a-02fc-415c-924b-3bc8940425c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory exists: C:\\Users\\MiraHatoum\\Downloads\\data\n",
            "Files in the directory: []\n",
            "No matching .csv files found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the directory path\n",
        "data_folder = r\"C:\\Users\\MiraHatoum\\Downloads\\data\"\n",
        "\n",
        "# Step 2: Check if the directory exists\n",
        "if not os.path.exists(data_folder):\n",
        "    print(f\"Directory does not exist: {data_folder}\")\n",
        "else:\n",
        "    print(f\"Directory exists: {data_folder}\")\n",
        "\n",
        "    # Step 3: List all files in the directory\n",
        "    files = os.listdir(data_folder)\n",
        "    print(\"Files in the directory:\", files)\n",
        "\n",
        "    # Step 4: Add missing .csv extensions if needed\n",
        "    for file in files:\n",
        "        file_path = os.path.join(data_folder, file)\n",
        "        if not file.endswith(\".csv\") and os.path.isfile(file_path):\n",
        "            new_file_path = file_path + \".csv\"\n",
        "            os.rename(file_path, new_file_path)\n",
        "            print(f\"Renamed: {file_path} to {new_file_path}\")\n",
        "\n",
        "    # Step 5: Use glob to find all .csv files\n",
        "    file_pattern = os.path.join(data_folder, \"*.csv\")\n",
        "    csv_files = glob.glob(file_pattern)\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"No matching .csv files found.\")\n",
        "    else:\n",
        "        print(f\"Found {len(csv_files)} CSV files:\")\n",
        "        for file in csv_files:\n",
        "            print(file)\n",
        "\n",
        "        # Step 6: Define the function to read and combine all CSV files\n",
        "        def read_all(library, file_paths):\n",
        "            df_list = []\n",
        "            for file in file_paths:\n",
        "                try:\n",
        "                    # Read each CSV file\n",
        "                    df = library.read_csv(\n",
        "                        file,\n",
        "                        index_col=None,\n",
        "                        header=0,  # Adjust to None if there's no header\n",
        "                        usecols=[0, 1, 2, 4, 5],  # Adjust column selection as needed\n",
        "                        skiprows=0  # Adjust if you need to skip rows\n",
        "                    )\n",
        "                    df_list.append(df)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file}: {e}\")\n",
        "            return library.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "        # Step 7: Combine CSV files using pandas\n",
        "        try:\n",
        "            df_cpu = read_all(pd, csv_files)\n",
        "            print(\"\\nCombined DataFrame:\")\n",
        "            print(df_cpu.head())\n",
        "        except Exception as e:\n",
        "            print(f\"Error combining CSV files: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP992BRFgoQN"
      },
      "source": [
        "Since Dask is made to be parallel, we do not need a for loop. It can read multiple files natively.\n",
        "\n",
        "The below code shows how to read data in parallel. This only sets up the process to read the files. we need to force Dask to *compute*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3gxFkalgoQP",
        "outputId": "e7883f4d-433a-4f10-95a5-a1e82d36414a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory exists: C:\\Users\\MiraHatoum\\Downloads\\data\n",
            "Files in the directory: []\n",
            "No matching .csv files found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Step 1: Define the folder path\n",
        "data_folder = r\"C:\\Users\\MiraHatoum\\Downloads\\data\"\n",
        "\n",
        "# Step 2: Verify the directory and list files\n",
        "if not os.path.exists(data_folder):\n",
        "    print(f\"Directory does not exist: {data_folder}\")\n",
        "else:\n",
        "    print(f\"Directory exists: {data_folder}\")\n",
        "    files = os.listdir(data_folder)\n",
        "    print(\"Files in the directory:\", files)\n",
        "\n",
        "    # Step 3: Use glob to find all .csv files\n",
        "    file_pattern = os.path.join(data_folder, \"*.csv\")\n",
        "    file_paths = glob.glob(file_pattern)\n",
        "\n",
        "    if not file_paths:\n",
        "        print(\"No matching .csv files found.\")\n",
        "    else:\n",
        "        print(f\"Found {len(file_paths)} CSV files:\")\n",
        "        for file in file_paths:\n",
        "            print(file)\n",
        "\n",
        "        # Step 4: Read files using Dask\n",
        "        try:\n",
        "            ddf_cpu = dd.read_csv(file_paths, usecols=[0, 1, 2, 4, 5], header=0, skipinitialspace=True)\n",
        "            print(\"Dask DataFrame loaded.\")\n",
        "\n",
        "            # Step 5: Compute the result\n",
        "            result = ddf_cpu.compute()\n",
        "            print(\"\\nCombined DataFrame:\")\n",
        "            print(result.head())\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading files with Dask: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxkSlBz0goQQ"
      },
      "source": [
        "Let's sample our data to confirm it had been read correctly. This time, we will only be working with the first three columns of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcmU9HQdgoQR",
        "outputId": "4ff54fb3-3207-46c3-a117-3a2345995882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory exists: C:\\Users\\MiraHatoum\\Downloads\\data\n",
            "Files in the directory: []\n",
            "No matching .csv files found.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Define the folder path\n",
        "data_folder = r\"C:\\Users\\MiraHatoum\\Downloads\\data\"\n",
        "\n",
        "# Check if directory exists\n",
        "if not os.path.exists(data_folder):\n",
        "    print(f\"Directory does not exist: {data_folder}\")\n",
        "else:\n",
        "    print(f\"Directory exists: {data_folder}\")\n",
        "    files = os.listdir(data_folder)\n",
        "    print(\"Files in the directory:\", files)\n",
        "\n",
        "    # Use glob to find .csv files\n",
        "    file_pattern = os.path.join(data_folder, \"*.csv\")\n",
        "    file_paths = glob.glob(file_pattern)\n",
        "\n",
        "    if not file_paths:\n",
        "        print(\"No matching .csv files found.\")\n",
        "    else:\n",
        "        print(f\"Found {len(file_paths)} CSV files:\")\n",
        "        for file in file_paths:\n",
        "            print(file)\n",
        "\n",
        "        # Create Dask DataFrame\n",
        "        try:\n",
        "            ddf_cpu = dd.read_csv(file_paths, usecols=[0, 1, 2], header=0, skipinitialspace=True)\n",
        "            print(\"Dask DataFrame created successfully.\")\n",
        "\n",
        "            # Sample data\n",
        "            print(\"Sampling data from Dask DataFrame:\")\n",
        "            print(ddf_cpu.head())\n",
        "\n",
        "            # Visualize DAG\n",
        "            print(\"Visualizing DAG...\")\n",
        "            ddf_cpu.visualize(filename='dag.png')\n",
        "            print(\"DAG saved as 'dag.png'.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating Dask DataFrame: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je4q51aHgoQR"
      },
      "source": [
        "How can Dask do this faster than regular pandas or cuDF? Under the hood, Dask is building a system of operations called a DAG. We can view this DAG with the [visualize](https://docs.dask.org/en/latest/graphviz.html) method."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}